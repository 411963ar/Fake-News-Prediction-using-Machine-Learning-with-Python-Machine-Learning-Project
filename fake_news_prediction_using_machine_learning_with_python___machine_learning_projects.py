# -*- coding: utf-8 -*-
"""Fake News Prediction using Machine Learning with Python | Machine Learning Projects.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B5hg7-gh_yrv2V1uTMiGgAA9Eu3GaUdB

Importing Libraries/dependencies
"""

import numpy as np #creating and manipulating arrays
import pandas as pd # data analysis on dataset
import re
from nltk.corpus import stopwords #removing common and less important words
from nltk.stem.porter import PorterStemmer #converting words to the root form by stemming
from sklearn.feature_extraction.text import TfidfVectorizer #converting text to numerical vector
from sklearn.model_selection import train_test_split #spliting data into train test splits
from sklearn.linear_model import LogisticRegression #selecting logistic regression model for binary classification as fake or not fake
from sklearn.metrics import accuracy_score  #to calculate accuracy of training and test data

# importing nltk and downloading the stopwords from nltk library

import nltk
nltk.download("stopwords")

print(stopwords.words("english")) # printing stop words of library and mentioning the language to be used

"""Data collection and preprocessing"""

df = pd.read_csv("/content/train.csv", on_bad_lines='skip')

df.head()

print(df.shape)

# counting no. of missing values in data

df.isnull().sum()

# since dataset contain 20k rows which is large enough  so I will just drop or replace the missing values as they are not very much,but we can also impute values as well

#replacing the  the null/missing values with empty string

df=df.fillna("")

df.isnull().sum()

# I will use title and author for prediction,so I am going to combine it

#merging the author name and news title

df["content"]= df["author"]+' '+df["title"]

df.head()

#we will just use the content column and labels for prediction

#separating the data and labels

x=df.drop(columns="label",axis=1)

y=df["label"]

print(x)
print(y)

# the three dots indicates that there are three other columns inbetween id and content column
#due to large sie its not printing but they are indeed stored in x while y contains only labels

"""Stemming"""

#reducing words to its root form by removing suffix

#we are reducing multiple words with same meaning to the root word to reduce complexity and improve predictions

#example: actoress,actor,acting all are rooted from act

port_stem= PorterStemmer() #loading the function into the variable

# we will create a function

def stemming(content): # defining function with a name stemming and providing input as feature content
    stemmed_content = re.sub('[^a-zA-Z]', ' ',content) # re.sub is a function of regular expression that search and substitute/replace text
    #  '[^a-z A-Z]' htis regular expression means replace anything other then alphabets from out content,negation^ of a-z A-Z will contain anything other then alphabets and will be replaced/remove by .sub function from out content, ' ' represents that the numbers,or commas or codes will be replaced
    stemmed_content = stemmed_content.lower()
    stemmed_content = stemmed_content.split()
    stemmed_content= [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words("english")]
    # this means we are using a variable port_stem in which we stored the function for stemming and we are running a for loop- to stem a world in stem_content that is not a stop word,we are excluding or removing stop words
    stemmed_content = ' '.join(stemmed_content) #joining the list of stemmed words again into a single string ' '
    return stemmed_content

df['content'] = df['content'].apply(stemming) #applying stemming function to df['content']

print(df['content'])

# Separating data and labels

X=df['content'].values #this function contain values will be stored in x, itwill extract the values
Y=df['label'].values #labels values will be stord in y

#we need this,the separation tht we did earlier is not necessary,

print(X)

print(Y)

# 1 means fake news and 0 means real news

#dots means so many values in between

"""Convertting the text into numerical number vector,so maodel can understand it"""

#tf means term frequency and idf means inverse document frequency,these frequencies represents how often a word appear in text,then it applies a numerical weight or values according to the repeatation that indicates the importance of that word

#idf identifies those repeating words that dont add value,example reviews about avenger movie,if every review contains word avengers,it means its repeating several time but actually dosent add value because its a movie name,so every review contains word avenger

vectorizer = TfidfVectorizer() #vectorizer=TfidfVectorizer() #storing function in variable
vectorizer.fit(X) # fitting the function on input data X

X= vectorizer.transform(X) # transforming the data X texts into feature vectors

print(X)

"""Spliting data into train test"""

X_train,X_test,Y_train,Y_test= train_test_split(X,Y, test_size=0.2,stratify=Y,random_state=2)

"""Training the model"""

model= LogisticRegression()
model.fit(X_train,Y_train)

"""Model evaluation"""

# Training data accuracy

X_train_prediction=model.predict(X_train) #model predicting training data

training_data_accuracy= accuracy_score(X_train_prediction,Y_train) #finding accuracy using training data and respective labels

print(training_data_accuracy)

#Test data accuracy

X_test_prediction= model.predict(X_test)
test_data_accuracy= accuracy_score(X_test_prediction,Y_test)
print(test_data_accuracy)

"""Building a predictive system"""

input_data=X_test[20] # we can take any data,for example we took test data values or feature vectors at 20th index
prediction= model.predict(input_data)
print(prediction)
if (prediction[0]==0): #prediction will be a single value array as 1 or 0 with only single index,so prediction[0] means first index and prediction[0]==0 or1 means if prediction at first or single index has value == to 0 or 1
  print("THE news is Real")
else:
  print("The news is Fake")

